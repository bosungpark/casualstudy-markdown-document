# AWS 스토리지 서비스 가이드

## 1. Amazon S3 (Simple Storage Service)

### 1.1 S3란 무엇인가

Amazon S3는 인터넷을 통해 접근 가능한 객체 스토리지 서비스다. '객체 스토리지'라는 개념이 핵심인데, 이는 파일 시스템이나 블록 스토리지와 근본적으로 다른 방식으로 데이터를 저장한다.

**파일 시스템 vs 객체 스토리지**

일반적인 파일 시스템(예: 윈도우 탐색기)은 디렉토리 트리 구조로 파일을 계층적으로 관리한다. 반면 객체 스토리지는 평면(flat) 구조다. 모든 객체가 같은 레벨에 존재하며, `folder/subfolder/file.txt` 같은 경로는 실제 폴더가 아니라 단순히 객체의 키(key) 이름일 뿐이다.

![파일 시스템 vs 객체 스토리지 비교](/images/file-vs-object-storage.png)

| 구분 | 파일 시스템 | 객체 스토리지 |
|------|------------|--------------|
| 구조 | 계층적 트리 구조 | 평면 구조 (키-값) |
| 메타데이터 | 제한적 (이름, 크기, 날짜) | 무제한 커스텀 메타데이터 |
| 수정 방식 | 일부 수정 가능 | 전체 교체만 가능 |
| 접근 방식 | 파일 경로 | HTTP URL |
| 확장성 | 단일 서버 한계 | 무제한 확장 |

**객체(Object)의 구성**

S3에 저장되는 각 객체는 세 가지 요소로 구성된다:
- 키(Key): 객체의 고유 식별자. 버킷 내에서 유일해야 한다.
- 값(Value): 실제 데이터. 최대 50TB까지 저장 가능.
- 메타데이터: 객체에 대한 추가 정보. 시스템 메타데이터(Content-Type 등)와 사용자 정의 메타데이터로 구분.

**S3 객체 크기 제한의 역사와 구조적 배경**

S3의 객체 크기 제한은 시간이 지나면서 발전해왔다:

| 시기 | 최대 객체 크기 | 주요 변화 |
|------|---------------|----------|
| 초기 S3 | 5GB | 단일 PUT 요청으로 업로드 |
| 2010년 이후 | 5TB | 멀티파트 업로드 API 도입 |
| 2025년 12월 | 50TB | 대규모 AI/ML 데이터셋 수요 대응 |

50TB 제한이 존재하는 구조적 이유는 멀티파트 업로드의 기술적 제약과 관련이 있다:
- 최대 파트 수: 10,000개
- 파트당 최대 크기: 5GB
- 이론적 최대 = 10,000 × 5GB = 50TB

실제로 이 제한은 대부분의 사용 사례에서 충분하며, AWS는 내부적으로 데이터를 청크 단위로 분산 저장하고 관리하기 때문에 무한정 큰 단일 객체를 허용하면 복구, 복제, 일관성 검증 등의 운영 복잡성이 기하급수적으로 증가한다.

![S3 객체 구성](/images/s3-object-structure.png)

### 1.2 S3의 내부 동작 원리

**데이터 분산 저장**

S3에 객체를 업로드하면, AWS는 해당 데이터를 자동으로 최소 3개의 가용 영역(AZ)에 분산 저장한다. 각 AZ는 물리적으로 분리된 데이터센터이므로, 하나의 데이터센터가 완전히 파괴되어도 데이터는 안전하다. 이것이 S3가 99.999999999%(11 nines) 내구성을 제공하는 방식이다.

![S3 데이터 분산 저장](/images/s3-data-distribution.png)

**결과적 일관성에서 강력한 일관성으로**

과거 S3는 '결과적 일관성(eventual consistency)' 모델을 사용했다. 객체를 업로드하거나 삭제한 직후 조회하면 이전 버전이 보일 수 있었다. 2020년 12월부터 S3는 모든 작업에 대해 '강력한 일관성(strong consistency)'을 제공한다. PUT 또는 DELETE 직후 GET 요청은 항상 최신 상태를 반환한다.

![일관성 모델 비교](/images/consistency-models.png)

**멀티파트 업로드**

100MB 이상의 대용량 파일은 멀티파트 업로드를 권장한다. 동작 방식:
- 파일을 여러 파트(최소 5MB)로 분할
- 각 파트를 병렬로 업로드
- 모든 파트 업로드 완료 후 S3가 자동으로 조립
- 실패한 파트만 재전송 가능 (전체 재시작 불필요)

![멀티파트 업로드 프로세스](/images/multipart-upload.png)

### 1.3 버킷(Bucket)과 객체 관리

**버킷의 특성**

버킷은 객체를 담는 컨테이너다. 중요한 특성들:
- 버킷 이름은 전 세계적으로 고유해야 한다 (모든 AWS 계정 통틀어)
- 버킷은 특정 리전에 생성되며, 객체는 명시적으로 복제하지 않는 한 해당 리전을 벗어나지 않는다
- 한 계정당 기본 100개 버킷 제한 (요청 시 증가 가능)
- 버킷 내 객체 수는 무제한

**버전 관리(Versioning)**

버전 관리를 활성화하면 같은 키로 객체를 덮어써도 이전 버전이 보존된다. 각 버전은 고유한 버전 ID를 가진다. 삭제 요청 시에도 실제로 삭제되지 않고 '삭제 마커(delete marker)'가 추가된다. 이전 버전 복원이 필요하면 해당 버전 ID를 지정해 조회하면 된다.

![S3 버전 관리](/images/s3-versioning.png)

**수명 주기(Lifecycle) 정책**

객체의 생애 주기를 자동화할 수 있다:
- 전환(Transition): 일정 기간 후 더 저렴한 스토리지 클래스로 자동 이동
- 만료(Expiration): 일정 기간 후 객체 자동 삭제
- 버전 정리: 이전 버전이나 불완전한 멀티파트 업로드 정리

### 1.4 스토리지 클래스와 비용 최적화

S3는 접근 빈도와 검색 시간 요구사항에 따라 여러 스토리지 클래스를 제공한다:

| 스토리지 클래스 | 용도 | 검색 시간 | 최소 저장 기간 |
|----------------|------|----------|---------------|
| S3 Standard | 자주 접근하는 데이터 | 즉시 | 없음 |
| S3 Intelligent-Tiering | 접근 패턴 예측 불가 | 즉시 | 없음 |
| S3 Standard-IA | 월 1회 정도 접근 | 즉시 | 30일 |
| S3 One Zone-IA | 재생성 가능한 비자주 접근 데이터 | 즉시 | 30일 |
| S3 Glacier Instant | 분기 1회 접근, 즉시 필요 | 밀리초 | 90일 |
| S3 Glacier Flexible | 연 1-2회 접근 | 분~시간 | 90일 |
| S3 Glacier Deep Archive | 거의 접근 안 함 (규정 준수용) | 12~48시간 | 180일 |

![S3 스토리지 클래스 비교](/images/s3-storage-classes.png)

**Intelligent-Tiering의 동작 방식**

접근 패턴을 예측하기 어려울 때 유용하다. S3가 각 객체의 접근 패턴을 모니터링하고:
- 30일간 접근 없으면 Infrequent Access 티어로 이동
- 90일간 접근 없으면 Archive Instant Access 티어로 이동 (선택적)
- 접근 시 자동으로 Frequent Access 티어로 복귀
- 티어 간 이동에 검색 비용 없음 (모니터링 비용만 발생)

### 1.5 접근 제어와 보안

**버킷 정책(Bucket Policy)**

JSON 형식의 리소스 기반 정책으로, 버킷과 그 안의 객체에 대한 접근을 제어한다. 다른 AWS 계정이나 익명 사용자에게 권한을 부여할 때 주로 사용한다.

**IAM 정책**

IAM 사용자, 그룹, 역할에 연결되는 자격 증명 기반 정책이다. 버킷 정책과 IAM 정책이 동시에 적용되면, 명시적 거부가 항상 우선하고, 그 다음 명시적 허용을 확인하며, 둘 다 없으면 기본 거부된다.

![S3 접근 제어 흐름](/images/s3-access-control.png)

**ACL(Access Control List)**

레거시 방식의 접근 제어다. AWS는 새 버킷에 대해 ACL 비활성화를 권장한다. 버킷 정책이 더 세밀한 제어를 제공하기 때문이다.

**퍼블릭 액세스 차단**

S3에는 4단계의 퍼블릭 액세스 차단 설정이 있다. 계정 레벨과 버킷 레벨 모두에서 설정 가능하며, 실수로 버킷을 공개하는 것을 방지한다. 프로덕션 환경에서는 특별한 이유가 없는 한 모든 퍼블릭 액세스 차단을 활성화해야 한다.

**데이터 암호화**

S3는 두 가지 레벨의 암호화를 제공한다:

**서버 측 암호화(SSE):** 데이터가 S3에 저장될 때 자동으로 암호화된다.
- SSE-S3: AWS가 관리하는 키 사용. 가장 간단.
- SSE-KMS: AWS KMS 키 사용. 키 사용 감사 로그 제공.
- SSE-C: 고객이 제공한 키 사용. 키 관리 책임은 고객.

**클라이언트 측 암호화:** 데이터를 S3로 보내기 전에 클라이언트에서 암호화. 전송 중에도 암호화된 상태 유지.

### 1.6 S3와 다른 서비스 연계

**정적 웹사이트 호스팅**

S3 버킷을 웹 서버처럼 사용할 수 있다. HTML, CSS, JavaScript, 이미지 등 정적 파일을 서빙하는 데 적합하다. 서버 측 스크립트(PHP, Node.js 등)는 실행 불가능하다.

정적 웹사이트란 서버 측 처리 없이 파일 그대로 브라우저에 전달되는 웹사이트다. React, Vue 같은 SPA도 빌드하면 정적 파일(HTML, JS, CSS)이 되므로 S3로 호스팅 가능하다. 단, API 호출은 별도 백엔드(Lambda, EC2 등)가 처리해야 한다.

**S3 정적 웹사이트 호스팅과 HTTPS**

S3 기본 엔드포인트(`https://bucket-name.s3.amazonaws.com`)는 HTTPS를 지원한다. AWS가 `*.s3.amazonaws.com` 와일드카드 인증서로 모든 버킷을 커버하기 때문이다.

반면 S3 정적 웹사이트 엔드포인트(`http://bucket-name.s3-website-region.amazonaws.com`)는 HTTPS를 지원하지 않는다. 기술적으로는 와일드카드 인증서로 커버 가능한 구조지만, AWS가 지원하지 않기로 한 설계 결정이다. 아마도 CloudFront와 함께 사용하도록 유도하려는 의도로 보인다.

실무에서 S3 정적 웹사이트를 직접 노출하는 경우는 거의 없고, CloudFront를 앞에 두어 HTTPS 종단점으로 사용한다. ACM(AWS Certificate Manager)에서 무료 SSL 인증서를 발급받아 CloudFront에 연결하면 된다.

![S3 + CloudFront 아키텍처](/images/s3-cloudfront-architecture.png)

**이벤트 알림**

객체 생성, 삭제 등의 이벤트 발생 시 다른 AWS 서비스를 트리거할 수 있다:
- Lambda: 이미지 업로드 시 자동 썸네일 생성
- SQS: 이벤트를 큐에 넣어 비동기 처리
- SNS: 이메일/SMS 알림 발송
- EventBridge: 복잡한 이벤트 라우팅

**교차 리전 복제(CRR)**

한 리전의 버킷에서 다른 리전의 버킷으로 객체를 자동 복제한다. 재해 복구, 지연 시간 단축, 규정 준수 요구사항 충족에 사용된다. 복제에는 버전 관리 활성화가 필수다.


## 2. Amazon EBS (Elastic Block Store)

### 2.1 EBS란 무엇인가

EBS는 EC2 인스턴스에 연결하는 블록 스토리지다. 물리 서버에 하드 드라이브나 SSD를 장착하는 것과 유사한 개념이다. S3가 HTTP로 접근하는 객체 스토리지라면, EBS는 파일 시스템을 만들어 마운트하는 블록 스토리지다.

**블록 스토리지의 특성**

블록 스토리지는 데이터를 고정 크기의 블록으로 나누어 저장한다:
- 파일의 일부만 수정 가능 (S3는 전체 교체만 가능)
- 운영체제가 파일 시스템(ext4, NTFS 등)을 직접 관리
- 랜덤 I/O에 최적화
- 데이터베이스, 부팅 볼륨 등에 적합

![블록 스토리지 개념](/images/block-storage-concept.png)

### 2.2 EBS 볼륨 타입

워크로드 특성에 따라 적절한 볼륨 타입을 선택해야 한다:

| 볼륨 타입 | 용도 | 최대 IOPS | 최대 처리량 |
|----------|------|-----------|------------|
| gp3 (범용 SSD) | 대부분의 워크로드 | 16,000 | 1,000 MB/s |
| gp2 (범용 SSD) | 이전 세대 범용 | 16,000 | 250 MB/s |
| io2/io2 Block Express | 고성능 DB | 256,000 | 4,000 MB/s |
| st1 (처리량 최적화 HDD) | 빅데이터, 로그 | 500 | 500 MB/s |
| sc1 (콜드 HDD) | 자주 접근 안 하는 대용량 | 250 | 250 MB/s |

**gp3 vs gp2**

gp3는 gp2의 후속 세대다. 핵심 차이점:
- gp2: IOPS가 볼륨 크기에 비례 (3 IOPS/GB). 높은 IOPS가 필요하면 큰 볼륨 구매 필요.
- gp3: IOPS와 처리량을 볼륨 크기와 독립적으로 프로비저닝. 기본 3,000 IOPS 제공.
- gp3가 대부분의 경우 20% 정도 저렴.

![EBS 볼륨 타입 선택 가이드](/images/ebs-volume-types.png)

**Provisioned IOPS (io2)**

일관된 고성능이 필요한 경우 사용한다. IOPS를 명시적으로 지정하고 AWS가 해당 성능을 보장한다. Oracle, SQL Server 같은 트랜잭션 집약적 데이터베이스에 적합하다.

### 2.3 EBS의 내부 동작

**네트워크 연결 스토리지**

EBS 볼륨은 EC2 인스턴스와 같은 물리 서버에 있지 않다. 네트워크를 통해 연결된다. 이 설계의 장점:
- 인스턴스 종료 후에도 데이터 유지
- 볼륨을 다른 인스턴스에 연결 가능
- 스냅샷으로 백업 용이

단점은 네트워크 지연이다. 극도로 낮은 지연이 필요한 경우 인스턴스 스토어(로컬 NVMe)를 고려할 수 있다.

![EBS 네트워크 연결 구조](/images/ebs-network-attached.png)

**단일 AZ 제약**

EBS 볼륨은 특정 가용 영역(AZ)에 존재한다. 해당 AZ의 EC2 인스턴스만 연결할 수 있다. 다른 AZ로 이동하려면 스냅샷을 생성하고 대상 AZ에서 새 볼륨을 생성해야 한다.

**Multi-Attach**

io2 볼륨은 Multi-Attach 기능을 지원한다. 하나의 볼륨을 동일 AZ 내 최대 16개의 Nitro 기반 인스턴스에 동시 연결할 수 있다. 클러스터 인식 파일 시스템이 필요하다 (일반 ext4나 XFS는 데이터 손상 위험).

**Chain Replication과 데이터 복제**

EBS는 내부적으로 Chain Replication을 사용하여 데이터 내구성을 보장한다. 데이터는 EC2 클라이언트에서 Primary 서버로, 다시 Replica 서버로 순차적으로 흐른다. 정상 운영 시에는 외부 조정(coordination) 없이 체인 복제가 독립적으로 동작한다. 그러나 장애가 발생하면—예를 들어 Primary 서버가 실패하면—복제 체인을 재구성해야 하며, 이 작업은 원자적(atomic)이고 순서가 보장되어야 한다.

![EBS Chain Replication](/images/ebs-chain-replication.png)

### 2.4 Physalia: EBS 컨트롤 플레인의 분산 메타데이터 시스템

EBS의 핵심 설계 과제 중 하나는 복제 체인 구성 정보를 어디에, 어떻게 저장할 것인가다. 2011년 us-east-1 리전 장애 이후, AWS는 기존의 중앙집중식 컨트롤 플레인이 가진 근본적 한계를 인식하고 새로운 접근법을 개발했다. 그 결과물이 Physalia다.

**왜 "수백만 개의 작은 데이터베이스"인가**

전통적인 접근법은 하나의 대규모 분산 데이터베이스로 모든 EBS 볼륨의 구성 정보를 관리하는 것이다. 그러나 이 방식은 치명적인 문제가 있다:

- **Blast Radius(피해 범위) 문제**: 단일 데이터베이스가 장애를 겪으면 모든 EBS 볼륨이 영향받는다
- **CAP 정리의 제약**: 네트워크 파티션 상황에서 일관성(Consistency)과 가용성(Availability)을 동시에 만족할 수 없다
- **부하 집중**: 대규모 장애 시 재구성 요청이 폭주하여 컨트롤 플레인이 가장 필요한 순간에 과부하된다

Physalia의 핵심 통찰은 "모든 키가 모든 클라이언트에게 가용할 필요는 없다"는 것이다. 각 EBS 볼륨의 구성 정보는 세 지점에서만 접근 가능하면 된다: 해당 볼륨을 사용하는 EC2 인스턴스, Primary 복제본, Replica 복제본.

![Physalia 아키텍처 개요](/images/physalia-overview.png)

**Colony와 Cell 아키텍처**

Physalia는 Colony-Cell 구조로 설계되었다:

| 개념 | 설명 |
|------|------|
| Colony | 하나의 데이터센터/AZ 내 전체 Physalia 배포 단위 |
| Cell | 단일 파티션 키(EBS 볼륨)를 담당하는 독립적인 분산 상태 머신 |
| Node | 물리 서버에서 실행되는 Physalia 프로세스. 하나의 노드가 여러 Cell에 참여 |

핵심은 각 Cell이 완전히 독립적이라는 점이다. Cell 간 조정이 없으므로 하나의 Cell 장애가 다른 Cell에 전파되지 않는다. 이것이 blast radius를 최소화하는 핵심 메커니즘이다.

![Physalia Colony-Cell 구조](/images/physalia-colony-cell.png)

**Cell 크기와 Paxos 합의**

각 Cell은 7개의 노드로 구성되며, Paxos 프로토콜을 사용하여 합의에 도달한다. 7이라는 숫자는 신중하게 선택되었다:

- **내구성**: Cell 크기가 커질수록 내구성이 지수적으로 향상
- **Tail Latency 내성**: 더 많은 노드가 있으면 일부 노드의 GC 일시정지 같은 지연을 더 잘 흡수
- **장애 내성 균형**: 소수의 비상관 장애에 대한 가용성과 대규모 장애 시 가용성 사이의 최적점

```
Cell 크기에 따른 가용성 특성:
- 작은 Cell: 소수 노드 장애에 취약하지만 50% 이상 장애 시 더 강건
- 큰 Cell: 소수 노드 장애에 강하지만 대규모 장애 시 전체 실패 가능성 증가
- 7 노드: 이 두 요구사항의 최적 균형점
```

**토폴로지 인식 배치(Topology-Aware Placement)**

Physalia의 가장 혁신적인 특징은 데이터센터의 네트워크와 전력 토폴로지를 이해하고 이를 활용한다는 점이다. Cell을 구성하는 노드 배치는 두 가지 상충하는 요구사항을 균형있게 만족해야 한다:

**근접성(Proximity)**: 노드들이 클라이언트와 가까워야 네트워크 파티션 발생 시 Cell이 클라이언트와 같은 쪽에 있을 확률이 높아진다. 네트워크 장치 수가 적을수록 파티션 가능성이 줄어든다.

**다양성(Diversity)**: 노드들이 너무 가까우면 안 된다. 동일 랙이나 전력 도메인에 모든 노드가 있으면 랙 장애 시 전체 Cell이 실패한다.

Physalia의 컨트롤 플레인은 실시간으로 이 트레이드오프를 관리한다:
- EC2 인스턴스가 이동하면 Cell도 점진적으로 재배치
- EBS 볼륨이 다른 인스턴스에 연결되면 Cell 노드를 새 위치에 가깝게 조정
- 각 노드 교체는 한 번에 하나씩 수행하여 가용성 유지

![토폴로지 인식 배치](/images/topology-aware-placement.png)

**Teaching과 Learning: 노드 동기화**

노드가 Cell에 합류하거나 재합류할 때 상태를 동기화하는 세 가지 모드가 있다:

- **Bulk 모드**: 새 노드용. Teacher 노드가 상태 머신의 전체 스냅샷을 전송
- **Log-based 모드**: 짧은 이탈 후 복귀 시. 로그 세그먼트를 전송하여 replay로 따라잡음
- **Whack-a-mole 모드**: 로그에 채울 수 없는 빈 공간이 있을 때. Learner가 빈 로그 위치에 no-op 트랜잭션을 제안. Paxos 프로토콜에 의해 이미 값이 있으면 그 값을 학습하고, 없으면 no-op이 채워짐

**Color 기반 운영 격리**

Physalia는 "Color"라는 개념으로 운영 위험을 분산한다. 각 Cell은 하나의 Color가 할당되고, Cell 내 모든 노드는 같은 Color다. 핵심 규칙:

- 다른 Color의 노드들은 서로 통신하지 않음
- 소프트웨어 배포는 Color 단위로 순차 진행
- 특정 Color에 문제가 생겨도 다른 Color의 Cell은 영향 없음

이 설계는 특히 "poison pill" 문제를 방지한다. Poison pill은 검증을 통과했지만 적용 시 오류를 일으키는 트랜잭션이다. 분산 상태 머신에서 모든 노드가 같은 순서로 같은 메시지를 처리하므로, poison pill이 들어오면 모든 노드가 동시에 실패할 수 있다. Color 분리로 이러한 상관 장애(correlated failure)의 blast radius를 제한한다.

![Color 기반 운영 격리](/images/color-based-isolation.png)

**Discovery Cache와 클라이언트 연결**

클라이언트가 자신의 Cell을 찾는 방식도 가용성을 고려해 설계되었다. Discovery Cache는 결과적 일관성(eventually consistent) 캐시로:

- 각 Cell이 주기적으로 자신의 구성 정보를 발행
- 클라이언트는 Discovery Cache에서 Cell 위치를 조회
- Cache 장애에 대비한 여러 완화 전략:
  - 클라이언트 측 캐싱
  - Forwarding 포인터 (Cell이 이동할 때 이전 노드가 새 위치를 가리킴)
  - Discovery Cache 자체의 복제

**Physalia 도입 효과**

Physalia 도입 전후의 EBS 가용성 차이는 극적이다. 논문에 따르면 Physalia 도입 후:
- EBS Primary가 구성 데이터에 접근할 때 오류율 0.05% 이상인 시간이 월 수십 시간에서 거의 0으로 감소
- 복제된 고객 데이터 대비 EBS 컨트롤 데이터의 가용성이 약 5,000배 높음
- 개별 EBS 볼륨 장애가 다른 볼륨에 전파되지 않음

**설계 교훈**

Physalia 논문은 분산 시스템 설계에 대한 중요한 교훈을 제공한다:

1. **Blast Radius 최소화를 설계 원칙으로**: 장애 발생 자체보다 장애 영향 범위가 더 중요할 수 있다
2. **CAP 정리의 실용적 해석**: 모든 키가 모든 클라이언트에게 가용할 필요 없음을 인식
3. **인프라 토폴로지 활용**: 네트워크/전력 토폴로지 지식으로 장애 상관성을 줄일 수 있음
4. **운영 격리**: Color 같은 메커니즘으로 배포 위험을 분산
5. **Postel의 원칙 재고**: 분산 상태 머신에서는 "관대하게 받아들이기"가 오히려 위험. 부분적으로만 이해하는 트랜잭션은 거부해야 함

### 2.5 스냅샷과 AMI

**스냅샷 동작 방식**

EBS 스냅샷은 S3에 저장되는 증분 백업이다:
- 첫 스냅샷: 전체 볼륨 복사
- 이후 스냅샷: 변경된 블록만 저장
- 스냅샷 삭제 시: 다른 스냅샷에서 참조하지 않는 블록만 삭제
- 복원: 어떤 스냅샷에서든 완전한 볼륨 복원 가능

![EBS 스냅샷 증분 백업](/images/ebs-snapshot-incremental.png)

**스냅샷과 성능**

스냅샷에서 복원된 볼륨은 즉시 사용 가능하지만, 첫 접근 시 S3에서 데이터를 가져오므로 지연이 발생한다. 성능이 중요하면 Fast Snapshot Restore를 활성화하거나, 볼륨 전체를 미리 읽어 초기화한다.

**AMI와의 관계**

AMI(Amazon Machine Image)는 EC2 인스턴스의 템플릿이다. AMI에는 EBS 스냅샷에 대한 참조가 포함된다. AMI에서 인스턴스를 시작하면 해당 스냅샷에서 루트 볼륨이 생성된다.

### 2.6 EBS 비용 구조

EBS 비용은 크게 세 가지로 구성된다:
- 볼륨 스토리지: 프로비저닝한 용량(GB)에 대해 월별 과금. 실제 사용량이 아님.
- 프로비저닝된 IOPS: io2 볼륨의 경우 IOPS당 추가 비용.
- 스냅샷: S3 스토리지 비용 (증분이므로 효율적).

gp3에서 추가 IOPS나 처리량을 프로비저닝하면 별도 비용이 발생한다. 기본 제공(3,000 IOPS, 125 MB/s)으로 충분한지 먼저 확인하는 것이 좋다.


## 3. 파일 스토리지 및 기타 서비스

### 3.1 Amazon EFS (Elastic File System)

**EFS vs EBS**

EFS는 여러 EC2 인스턴스가 동시에 접근할 수 있는 공유 파일 시스템이다. EBS는 단일 인스턴스 연결이 기본인 반면, EFS는 수천 개의 인스턴스가 동시에 마운트할 수 있다.

| 특성 | EBS | EFS |
|-----|-----|-----|
| 접근 범위 | 단일 인스턴스 (기본) | 다중 인스턴스 |
| AZ 범위 | 단일 AZ | 리전 전체 (다중 AZ) |
| 프로토콜 | 블록 (직접 연결) | NFS v4.1 |
| 용량 | 미리 프로비저닝 | 자동 확장/축소 |
| 비용 모델 | 프로비저닝 용량 | 실제 사용량 |

![EBS vs EFS 비교](/images/ebs-vs-efs.png)

**EFS 동작 방식**

EFS는 NFS(Network File System) 프로토콜을 사용한다. 각 AZ에 마운트 타겟을 생성하고, EC2 인스턴스는 해당 마운트 타겟을 통해 파일 시스템에 접근한다. 데이터는 자동으로 다중 AZ에 복제되어 내구성을 보장한다.

**성능 모드**
- 범용(General Purpose): 대부분의 워크로드에 적합. 지연 시간 우선.
- 최대 I/O: 높은 병렬 처리가 필요한 빅데이터 워크로드. 처리량 우선, 지연 시간 약간 높음.

**처리량 모드**
- 버스팅: 저장 용량에 비례한 기본 처리량 + 크레딧 기반 버스트.
- 프로비저닝: 처리량을 명시적으로 지정. 일정한 높은 처리량 필요 시.
- Elastic: 워크로드에 따라 자동 조절. 예측 불가능한 패턴에 적합.

**스토리지 클래스**

EFS도 S3처럼 스토리지 클래스가 있다:
- Standard: 자주 접근하는 데이터
- Infrequent Access (IA): 자주 접근하지 않는 데이터 (최대 92% 저렴)
- 수명 주기 정책으로 자동 전환 가능

### 3.2 Amazon FSx

FSx는 특정 파일 시스템이 필요할 때 사용하는 완전 관리형 서비스다.

**FSx for Windows File Server**

Windows 네이티브 SMB 프로토콜을 지원한다. Active Directory 통합, DFS(Distributed File System), 섀도 복사본 등 Windows 파일 서버 기능을 제공한다. Windows 기반 애플리케이션을 AWS로 마이그레이션할 때 유용하다.

**FSx for Lustre**

Lustre는 HPC(High Performance Computing)에 최적화된 병렬 파일 시스템이다. 머신러닝 훈련, 금융 모델링, 동영상 렌더링 등 대규모 컴퓨팅 워크로드에 적합하다. S3와 네이티브 통합을 제공하여 S3 데이터를 Lustre 파일 시스템으로 자동 로드하고 결과를 다시 S3로 저장할 수 있다.

**FSx for NetApp ONTAP**

NetApp의 ONTAP 파일 시스템을 AWS에서 완전 관리형으로 제공한다. NFS, SMB, iSCSI 프로토콜을 모두 지원한다. 온프레미스 NetApp 환경과의 호환성이 필요할 때 선택한다.

**FSx for OpenZFS**

ZFS 파일 시스템 기반이다. 데이터 중복 제거, 압축, 스냅샷 등 ZFS의 고급 기능을 AWS에서 사용할 수 있다.

### 3.3 AWS Transfer Family

SFTP, FTPS, FTP 프로토콜로 S3나 EFS에 파일을 전송할 수 있게 해주는 서비스다.

**사용 사례**
- 레거시 파일 전송 워크플로우를 클라우드로 마이그레이션
- 파트너사가 기존 FTP 클라이언트를 계속 사용하면서 S3에 직접 업로드
- 기존 인증 시스템(AD, LDAP) 통합

**동작 방식**

Transfer Family 서버를 생성하면 AWS가 프로토콜 엔드포인트를 제공한다. 사용자 인증 후 전송된 파일은 지정된 S3 버킷이나 EFS 파일 시스템에 저장된다. 서버 관리, 스케일링, 패칭은 AWS가 담당한다.

### 3.4 AWS Backup

여러 AWS 서비스의 백업을 중앙에서 관리하는 서비스다.

**지원 서비스**

EBS, EFS, RDS, DynamoDB, Aurora, Storage Gateway, FSx, EC2 등 대부분의 스토리지/데이터베이스 서비스를 지원한다.

**백업 계획(Backup Plan)**

백업 정책을 정의한다:
- 백업 빈도: 매일, 매주, 매월 등
- 백업 윈도우: 백업 실행 시간대
- 수명 주기: 콜드 스토리지로 전환 시점, 삭제 시점
- 교차 리전 복사: DR을 위한 다른 리전 복제

**백업 볼트(Backup Vault)**

백업이 저장되는 컨테이너다. 볼트 잠금(Vault Lock)을 사용하면 WORM(Write Once Read Many) 정책을 적용하여 백업 삭제를 방지할 수 있다. 규정 준수 요구사항 충족에 유용하다.

### 3.5 AWS DataSync

온프레미스 스토리지와 AWS 간, 또는 AWS 서비스 간 데이터 이동을 자동화한다.

**vs 일반 복사 도구**

rsync나 일반 복사 도구 대비 장점:
- 자동 암호화 및 데이터 검증
- 네트워크 최적화로 빠른 전송 (최대 10Gbps)
- 대역폭 조절 기능
- 스케줄링 및 모니터링 내장
- 증분 전송 지원

**에이전트 기반 동작**

온프레미스 데이터 전송 시 DataSync 에이전트를 배포한다. 에이전트가 로컬 스토리지에서 데이터를 읽어 AWS로 전송한다. AWS 서비스 간 전송은 에이전트 없이 가능하다.

### 3.6 AWS Snow Family

네트워크를 통한 데이터 전송이 비실용적일 때 물리적 디바이스로 데이터를 이동한다.

**디바이스 종류**

| 디바이스 | 스토리지 용량 | 용도 |
|---------|-------------|------|
| Snowcone | 8-14 TB | 엣지 컴퓨팅, 소규모 전송 |
| Snowball Edge Storage | 80 TB | 페타바이트 마이그레이션 |
| Snowball Edge Compute | 42-80 TB | 엣지 컴퓨팅 + 스토리지 |
| Snowmobile | 100 PB | 엑사바이트급 마이그레이션 |

**사용 프로세스**
- 콘솔에서 작업(Job) 생성
- AWS가 디바이스 배송
- 온프레미스에서 데이터 복사
- AWS로 반송
- AWS가 데이터를 S3로 가져오기

**언제 Snow를 사용하나**

경험적으로, 1Gbps 회선으로 1주일 이상 걸리는 데이터(약 60TB 이상)면 Snow를 고려한다. 100Mbps 회선이면 약 6TB부터 Snow가 유리할 수 있다.


## 4. 서비스 선택 가이드

### 4.1 의사결정 흐름

**질문 1: 어떤 접근 방식이 필요한가?**
- HTTP/REST API로 접근 → S3
- 파일 시스템으로 마운트 → EFS, FSx
- 블록 디바이스로 연결 → EBS

**질문 2: 공유가 필요한가?**
- 단일 인스턴스 전용 → EBS
- 여러 인스턴스가 동시 접근 → EFS, FSx
- 애플리케이션/서비스 간 공유 → S3

**질문 3: 어떤 프로토콜이 필요한가?**
- NFS → EFS 또는 FSx for ONTAP
- SMB (Windows) → FSx for Windows
- Lustre (HPC) → FSx for Lustre
- SFTP/FTP → Transfer Family + S3

![AWS 스토리지 서비스 선택 플로우차트](/images/storage-decision-flowchart.png)

### 4.2 비용 최적화 체크리스트

- S3: 접근 패턴에 맞는 스토리지 클래스 사용, Intelligent-Tiering 고려
- EBS: 실제 필요한 IOPS/처리량 분석, gp3 우선 검토, 미사용 볼륨 정리
- EFS: IA 스토리지 클래스 활용, 처리량 모드 최적화
- 스냅샷/백업: 보존 정책 설정, 불필요한 스냅샷 정리
- 데이터 전송: 같은 AZ 내 전송 무료, 리전 간 전송 비용 주의

### 4.3 아키텍처 패턴

**웹 애플리케이션**

EC2/ECS에 애플리케이션 배포, 정적 자산(이미지, CSS, JS)은 S3 + CloudFront로 서빙한다. 사용자 업로드 파일은 S3에 저장하고, 데이터베이스 볼륨은 EBS gp3 또는 io2를 사용한다.

**컨테이너 기반 마이크로서비스**

컨테이너는 기본적으로 상태 비저장(stateless)으로 설계한다. 영속 데이터는 EFS(공유 필요 시) 또는 S3(객체 저장)에 저장하고, 데이터베이스는 RDS 또는 DynamoDB를 사용한다.

**빅데이터/ML 파이프라인**

원시 데이터와 처리 결과는 S3 Data Lake에 저장한다. 고성능 처리가 필요한 단계는 FSx for Lustre를 S3와 연계하여 사용한다. 모델 훈련 결과물과 체크포인트도 S3에 저장한다.


## 5. 분산 시스템 설계 원칙: AWS 스토리지에서 배우는 교훈

AWS 스토리지 서비스들의 내부 설계를 살펴보면 대규모 분산 시스템 설계에 대한 공통된 원칙들이 드러난다.

### 5.1 Blast Radius 최소화

Physalia 사례에서 보듯, AWS는 "장애 발생 빈도"보다 "장애 영향 범위"를 더 중요하게 여긴다. 하나의 대규모 시스템보다 수백만 개의 독립적인 작은 시스템이 전체 가용성 측면에서 유리할 수 있다. 이 원칙은:

- EBS의 Cell 기반 아키텍처
- S3의 파티션별 독립 복제
- DynamoDB의 파티션 분리

에서 일관되게 적용된다.

### 5.2 토폴로지 인식 설계

네트워크와 전력 인프라의 물리적 구조를 이해하고 이를 설계에 반영하면:
- 네트워크 파티션 시 클라이언트와 서비스가 같은 쪽에 있을 확률 증가
- 로컬 장애가 글로벌 장애로 확대되는 것을 방지
- 지연 시간 최적화

### 5.3 운영과 설계의 통합

시스템 설계만으로는 고가용성을 달성할 수 없다. Physalia의 Color 기반 배포, 점진적 롤아웃, SimWorld 테스팅 등은 운영 관행이 설계의 핵심 부분임을 보여준다.

### 5.4 일관성 모델의 실용적 선택

모든 것에 강력한 일관성이 필요하지는 않다:
- Physalia의 Cell 데이터: 강력한 일관성 필수 (복제 프로토콜 정확성)
- Discovery Cache: 결과적 일관성으로 충분 (가용성 우선)
- S3 메타데이터: 2020년부터 강력한 일관성 (사용자 기대 충족)

---

## 참고 문헌

- Brooker, M., Chen, T., & Ping, F. (2020). Millions of Tiny Databases. NSDI'20.
- AWS Documentation: Amazon S3, EBS, EFS User Guides
- AWS re:Invent 발표 자료들