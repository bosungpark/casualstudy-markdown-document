- The need for guardrails with large language models in high-risk contexts: https://www.nature.com/articles/s41598-025-09138-0
- Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations: https://arxiv.org/pdf/2312.06674
- Beyond Linear Probes: Dynamic Safety Monitoring for Language Models: https://arxiv.org/abs/2509.26238
- Evaluation-Driven Development and Operations of LLM Agents (EDDOps): https://arxiv.org/html/2411.13768v3
- Architecting software monitors for control-flow anomaly detection through large language models and conformance checking: https://arxiv.org/abs/2511.10876
- Safeguarding Large Language Models: A Survey: https://arxiv.org/abs/2406.02622
- SoK: Evaluating Jailbreak Guardrails for Large Language Models: https://arxiv.org/abs/2506.10597
- GuardReasoner: Towards Reasoning-based LLM Safeguards: https://arxiv.org/abs/2501.18492
- MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks: https://arxiv.org/abs/2409.17699
- Effective Concurrency Testing for Go via Directional Primitive-constrained Interleaving Exploration: https://chao-peng.github.io/publication/ase23/?utm_source=chatgpt.com
- A Study of Real-World Data Races in Golang: https://arxiv.org/abs/2204.00764?utm_source=chatgpt.com
- An Empirical Study of Messaging Passing Concurrency in Go Projects: https://kar.kent.ac.uk/71491/?utm_source=chatgpt.com
- Hero: On the Chaos When PATH Meets Modules: https://arxiv.org/abs/2102.12105?utm_source=chatgpt.com
- Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector (HaluAgent) — (EMNLP/arXiv, 2024): https://arxiv.org/abs/2406.11277?utm_source=chatgpt.com
- Light-Weight Hallucination Detection using Contrastive Learning — (ACL SRW, 2025): https://aclanthology.org/2025.acl-srw.44.pdf?utm_source=chatgpt.com
- Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection — (arXiv, 2025): https://arxiv.org/abs/2511.05854?utm_source=chatgpt.com
- LLM-Check / Studies on Using Multiple Evaluators (LLMs) as Judges — (OpenReview / 2024–2025): https://openreview.net/pdf?id=LYx4w3CAgy&utm_source=chatgpt.com
  