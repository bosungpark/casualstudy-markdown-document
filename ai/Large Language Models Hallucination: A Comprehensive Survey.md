# Large Language Models Hallucination: A Comprehensive Survey

## 출처
- **링크**: https://arxiv.org/abs/2510.06265
- **저자**: Aisha Alansari, Hamzah Luqman (KFUPM)
- **발표일**: 2025년 10월 9일 (v2)
- **분야**: Computation and Language (cs.CL)

---

## AI 요약

### 1. 연구 배경 및 목적

이 서베이는 대규모 언어 모델(LLM)의 환각(Hallucination) 현상에 대한 포괄적 리뷰를 제공한다. 환각이란 LLM이 유창하고 문법적으로 올바르지만, 사실적으로 부정확하거나 외부 증거로 뒷받침되지 않는 콘텐츠를 생성하는 현상을 의미한다. 이 논문은 기존 서베이들과 차별화되는 세 가지 핵심 기여를 제시한다:

1. **원인 분석의 심층화**: LLM 개발 전체 라이프사이클(데이터 수집 → 아키텍처 설계 → 추론)에 걸친 환각 원인의 체계적 분석
2. **탐지 기법 분류체계**: 5가지 범주(검색 기반, 불확실성 기반, 임베딩 기반, 학습 기반, 자기 일관성 기반)로 구성된 탐지 접근법의 구조화된 분류체계 제안
3. **완화 전략 분류체계**: 4가지 범주(프롬프트 기반, 검색 기반, 추론 기반, 모델 중심 훈련/적응 기반)로 구성된 완화 전략의 체계적 분류

### 2. 환각의 유형 분류

#### 2.1 내재적 환각 (Intrinsic Hallucination)
- 생성된 출력이 소스 콘텐츠와 직접 모순되는 경우
- 원본 입력과 명백히 충돌하는 정보 생성

#### 2.2 외재적 환각 (Extrinsic Hallucination)
- 소스에서 검증하거나 추론할 수 없는 정보 생성
- 모순은 아니지만 근거 없는 주장 포함

### 3. 환각 원인 분석 (LLM 개발 라이프사이클 기반)

#### 3.1 데이터 수집 및 준비 단계
- **데이터 품질 문제**: 웹 스케일의 필터링되지 않은 코퍼스는 불일치, 편향, 오래되거나 허위 정보를 포함
- **중복 데이터**: 동일 정보의 반복 노출로 인한 과신 유발
- **출처 편향**: 특정 출처나 관점에 대한 과대표현

#### 3.2 모델 아키텍처 단계
- **어텐션 메커니즘 한계**: 긴 컨텍스트에서 관련 정보 식별 실패
- **지식 표현 제약**: 파라미터에 인코딩된 지식의 불완전성
- **위치 편향**: 입력 시퀀스 내 특정 위치에 대한 과도한 의존

#### 3.3 사전 훈련 단계
- **지름길 학습 (Shortcut Learning)**: 데이터의 표면적, 비강건 패턴 학습으로 분포 외 데이터에서 환각 발생
- **교사 강제 학습 (Teacher Forcing)**: 훈련 시 완벽한 컨텍스트 기반 예측 vs 추론 시 자체 생성 토큰 기반 예측의 불일치 (노출 편향)
- **눈덩이 효과 (Snowball Effect)**: 초기 토큰 오류가 연쇄적 오류로 확산
- **부정적 예시 부족**: 잘못된 출력에 대한 교정 피드백 부재

#### 3.4 미세 조정 단계
- **과적합**: 좁은 도메인 데이터에 대한 과도한 민감성
- **RLHF의 이중성**: 인간 선호 정렬 시 사실적 정확성보다 유창성 선호 가능성
- **보상 해킹**: 실제 품질 향상 없이 보상 신호 최적화

#### 3.5 추론 단계
- **디코딩 무작위성**: 높은 temperature나 차선 샘플링이 비개연적 토큰 선택 유도
- **컨텍스트 길이 제한**: 긴 입력에서 관련 정보 손실
- **프롬프트 모호성**: 불명확하거나 복잡한 프롬프트가 오해 유발

### 4. 환각 탐지 기법 분류체계

#### 4.1 검색 기반 탐지 (Retrieval-based Detection)
- **외부 지식 검색**: 생성된 콘텐츠를 신뢰할 수 있는 외부 소스와 대조
- **FACTSCORE**: 생성 텍스트를 원자적 사실로 분해하여 지식 소스와의 일치율 계산
- **NLI 기반 검증**: 자연어 추론 모델로 생성 텍스트와 검색된 증거 간 함의 관계 평가

#### 4.2 불확실성 기반 탐지 (Uncertainty-based Detection)
- **토큰 확률 분석**: 낮은 확률 토큰이 환각 가능성 시사
- **의미론적 밀도 (Semantic Density)**: 응답별 신뢰도/불확실성 점수 제공
- **엔트로피 추정**: 출력 분포의 엔트로피로 모델 확신도 측정

#### 4.3 임베딩 기반 탐지 (Embedding-based Detection)
- **의미적 유사도**: 생성 텍스트와 소스 간 임베딩 유사도 측정
- **BERTScore**: 토큰 수준 임베딩 매칭으로 의미적 일관성 평가
- **내부 표현 분석**: 모델 내부 활성화 패턴에서 환각 신호 탐지

#### 4.4 학습 기반 탐지 (Learning-based Detection)
- **분류기 훈련**: 환각/비환각 레이블 데이터로 탐지 모델 학습
- **Lookback Lens**: 어텐션 분석 기반 비지도 탐지, 모델이 이전 컨텍스트 vs 자체 출력에 얼마나 주의를 기울이는지 측정
- **HaluAgent**: 다단계 탐지 파이프라인 통합 자율 에이전트

#### 4.5 자기 일관성 기반 탐지 (Self-consistency-based Detection)
- **SelfCheckGPT**: 동일 프롬프트에 대한 다중 응답 생성 후 일관성 검사
- **핵심 원리**: 사실적 진술은 샘플 간 일관적, 환각은 샘플 간 모순
- **제로 리소스 접근**: 외부 지식베이스 없이 블랙박스 환경에서도 적용 가능

### 5. 환각 완화 전략 분류체계

#### 5.1 프롬프트 기반 완화 (Prompt-based Mitigation)
- **Chain-of-Thought (CoT)**: 단계별 추론 유도로 논리적 일관성 향상
- **자기 검증 (Self-Verification)**: 모델이 자체 출력 검증하도록 프롬프팅
- **출처 명시 프롬프팅**: "~에 따르면" 형식으로 근거 있는 응답 유도
- **불확실성 표현 허용**: 확신 없을 때 "모르겠다" 응답 허용

#### 5.2 검색 기반 완화 (Retrieval-based Mitigation)
- **RAG (Retrieval-Augmented Generation)**: 추론 시점에 관련 문서 검색하여 컨텍스트 제공
- **RETRO**: 훈련 중 검색 모듈 통합
- **Self-RAG**: 자기 반성을 통한 검색, 생성, 비평의 통합 학습
- **반복적 RAG (Iterative RAG)**: 다중 라운드 검색 및 생성

#### 5.3 추론 기반 완화 (Reasoning-based Mitigation)
- **대조적 디코딩 (Contrastive Decoding)**: 기준 모델과의 출력 비교로 덜 사실적인 완성 억제
- **불확실성 인식 빔 서치**: 불확실성 점수 기반 디코딩 경로 선택
- **자기 개선 (Self-Refinement)**: 생성 후 반복적 수정
- **Chain-of-Verification (CoVe)**: 검증 질문 생성 및 검토

#### 5.4 모델 중심 훈련/적응 기반 완화 (Model-centric Training & Adaptation)
- **RLHF (Reinforcement Learning from Human Feedback)**: 인간 선호 기반 모델 정렬, GPT-4는 RLHF로 사실 오류 40% 감소
- **DPO (Direct Preference Optimization)**: RLHF보다 단순한 선호 최적화, LLaMA-2에서 58% 사실 오류 감소
- **지식 편집 (Knowledge Editing)**: 전체 재훈련 없이 특정 지식의 정밀 수정
- **사실 기반 미세 조정**: 고품질 큐레이팅 데이터셋으로 미세 조정

### 6. 평가 벤치마크 및 메트릭

#### 6.1 주요 벤치마크
| 벤치마크 | 특징 | 평가 대상 |
|---------|------|----------|
| TruthfulQA | 인간의 일반적 오해 측정 | 사실성 (포화 문제 존재) |
| HaluEval | 35K 환각 샘플 포함 | QA, 대화, 요약 환각 탐지 |
| FACTSCORE | 원자적 사실 단위 평가 | 장문 텍스트 사실 정밀도 |
| FEVER | 주장 검증 | 팩트체킹 능력 |
| SelfCheckGPT | 자기 일관성 기반 | 제로 리소스 환각 탐지 |
| SimpleQA | 단순 사실 질문 | 기본 사실 지식 |

#### 6.2 평가 메트릭 한계
- **BLEU/ROUGE**: 사실적 일관성 포착 실패
- **TruthfulQA**: 훈련 데이터 포함으로 인한 포화, 일부 골드 답변 오류
- **자동 메트릭**: 도메인 외 데이터 및 저자원 언어에서 성능 저하

### 7. 미해결 과제 및 향후 연구 방향

#### 7.1 현재의 한계점
- **다국어 환각**: 저자원 언어에서의 환각 탐지/완화 연구 부족
- **멀티모달 환각**: 비전-언어 모델에서의 교차 모달 환각
- **지식 경계 이해**: 모델이 "모른다"고 인식하는 메커니즘 불명확
- **인센티브 문제**: 현재 평가 체계가 자신있는 추측을 보상

#### 7.2 유망한 연구 방향
- **보정 인식 평가 (Calibration-aware Evaluation)**: 불확실성 신호에 대한 보상
- **거부 학습 (Refusal Training)**: 불확실할 때 응답 거부를 학습된 정책으로
- **하이브리드 파이프라인**: RAG + RLHF + 가드레일 결합 (96% 환각 감소 달성)
- **심층 방어 전략 (Defense-in-Depth)**: 다층적 완화 접근법

---

## 내가 얻은 인사이트

### 1. 환각의 본질적 이해: 확률적 토큰 예측의 한계

이 논문에서 가장 중요한 통찰은 **환각이 LLM의 버그가 아니라 본질적 특성**이라는 점이다. LLM은 근본적으로 다음 토큰을 예측하는 확률적 생성기(Pθ(y|x))로서, 사실적으로 근거 있는 시퀀스보다 통계적으로 그럴듯한 시퀀스에 더 높은 확률을 부여할 때 환각이 발생한다. 이는 OpenAI의 2025년 9월 논문에서 강조한 **"인센티브 문제"**와 맥락을 같이 한다: 현재의 훈련 목표(next-token prediction)와 평가 벤치마크가 불확실성의 정확한 전달보다 그럴듯해 보이는 텍스트를 보상하기 때문에, 모델은 "자신있게 추측하는 것"을 학습하게 된다.

**실무적 함의**: 환각 문제 해결을 위해서는 더 많은 데이터나 영리한 프롬프트보다 **근본적인 인센티브 구조 변경**이 필요하다. 보정 인식 메트릭과 거부를 유효한 결과로 취급하는 보상 체계 도입이 핵심이다.

### 2. 라이프사이클 전반의 원인 분석이 주는 시스템적 관점

논문이 제시하는 6단계 개발 파이프라인(데이터 수집 → 아키텍처 → 사전훈련 → 미세조정 → 평가 → 추론) 기반 원인 분석은 환각을 **단일 지점의 실패가 아닌 누적적 시스템 문제**로 이해하게 해준다.

특히 주목할 점은 **교사 강제 학습과 노출 편향**의 관계다. 훈련 시에는 완벽한 ground truth 컨텍스트를 보지만, 추론 시에는 자체 생성한 토큰(오류 포함 가능)을 기반으로 예측해야 하는 근본적 불일치가 존재한다. 초기 토큰 오류가 **눈덩이 효과**로 확산되는 메커니즘은 고엔트로피 세그먼트에서 특히 두드러진다.

**실무적 함의**: 환각 완화는 단일 기법이 아닌 **각 단계별 맞춤 전략의 조합**이 필요하다. 데이터 품질 관리(전처리), 아키텍처 개선(훈련), RAG/프롬프팅(추론) 등을 병렬적으로 적용해야 한다.

### 3. 탐지-완화 간의 상호 의존성과 통합 파이프라인

논문의 탐지 분류체계(5범주)와 완화 분류체계(4범주)를 교차 분석하면, **탐지와 완화가 별개의 문제가 아니라 상호 보완적 관계**임을 알 수 있다.

예를 들어:
- **검색 기반 탐지 → RAG 기반 완화**: 외부 지식 검색으로 환각을 탐지하는 동일한 메커니즘이 완화에도 활용
- **불확실성 기반 탐지 → 불확실성 인식 디코딩**: 토큰 확률 분석이 탐지뿐 아니라 디코딩 전략 수정에도 적용
- **자기 일관성 탐지 → 자기 개선 완화**: SelfCheckGPT의 다중 샘플링이 탐지와 수정 모두에 활용

**실무적 함의**: 가장 효과적인 접근은 **탐지-완화 통합 파이프라인** 구축이다. 예: RAG로 컨텍스트 제공 → 생성 → SelfCheck로 일관성 검증 → CoVe로 자기 검증 → 필요시 재생성.

### 4. 모델 크기 확장의 역설과 정렬의 중요성

논문에서 인용된 TruthfulQA 연구 결과는 **더 큰 모델이 반드시 더 사실적이지 않다**는 역설을 보여준다. 대형 모델은 훈련 데이터의 패턴(인간의 일반적 오해 포함)을 더 자신있게 재현할 수 있어, 특정 질문 범주에서 오히려 덜 진실된 답변을 생성할 수 있다.

반면 RLHF를 적용한 GPT-4는 40% 사실 오류 감소, DPO를 적용한 LLaMA-2는 58% 감소를 달성했다. 이는 **스케일링보다 정렬(alignment)이 사실성에 더 결정적**임을 시사한다.

**실무적 함의**: 예산 제약 하에서 모델 크기 확장보다 **정렬 기법 투자**가 환각 감소에 더 효율적일 수 있다. 특히 DPO는 RLHF보다 단순하고 비용 효율적이면서 유사한 효과를 달성할 수 있어 실용적 대안이 된다.

### 5. RAG의 가능성과 한계: 검색 실패 모드

RAG는 가장 널리 채택된 환각 완화 기법이지만, 논문은 그 한계를 명확히 지적한다:

1. **검색 단계 실패**: 쿼리-문서 의미적 불일치로 관련 문서 검색 실패(낮은 recall) 또는 노이즈/무관/모순 문서 검색(낮은 precision)
2. **생성 단계 실패**: 완벽한 검색에도 불구하고 모델이 컨텍스트 무시하고 파라메트릭 지식 사용, 또는 검색된 텍스트 오해석
3. **과도한 외삽**: 증거에 명시적으로 지원되지 않는 그럴듯한 세부사항 생성 (장문 생성에서 흔함)

**실무적 함의**: RAG 시스템 구축 시 **검색 품질 모니터링**이 필수다. 하이브리드 검색(dense + sparse), 리랭킹 모델 적용, 그리고 컨텍스트가 불충분할 때 "정보 없음"을 명시하도록 프롬프팅하는 것이 중요하다. 또한 **근거성(groundedness)과 인용 정확도 메트릭**을 지속적으로 측정해야 한다.

### 6. 벤치마크 포화 문제와 동적 평가의 필요성

TruthfulQA의 사례는 **정적 벤치마크의 구조적 한계**를 보여준다. 훈련 데이터에 포함되면서 포화되고, 일부 골드 답변 오류, 그리고 불필요하게 엄격한 메트릭이 문제다.

HalluLens 논문의 제안처럼 **동적으로 재생성 가능한 평가 데이터**가 필요하다. 또한 환각(사실성에 대한 불충실)과 사실성(모델의 지식 범위)을 구분하는 **전용 평가 태스크**가 필요하다.

**실무적 함의**: 프로덕션 환경에서는 공개 벤치마크에 과적합하지 않도록 **자체 홀드아웃 테스트셋**을 유지하고, Vectara HHEM처럼 비공개 데이터셋 기반 평가를 병행해야 한다.

### 7. 추론 시점 개입의 실용성: 재훈련 없는 완화

논문에서 소개된 여러 기법들(SelfCheckGPT, Lookback Lens, ICQ 프레임워크)은 **모델 재훈련 없이 추론 시점에서 환각을 탐지하고 완화**할 수 있다는 점에서 실무적으로 매우 가치 있다.

특히 **Substantive-word Uncertainty Score (SUScore)**는 명사, 동사, 숫자 등 의미적으로 중요한 토큰의 불확실성에 집중하여, 단순 토큰 확률 분석보다 더 정교한 환각 탐지를 가능하게 한다.

**실무적 함의**: 이미 배포된 모델에 대해 **추론 파이프라인에 탐지 레이어를 추가**하는 것만으로도 상당한 환각 감소 효과를 얻을 수 있다. 프롬프트 엔지니어링, 다중 샘플링 + 일관성 검사, 검색 증강의 조합이 최소 비용으로 최대 효과를 낼 수 있는 실용적 접근이다.

### 8. 종합: 다층 방어 전략의 필수성

Stanford 2024년 연구에서 RAG + RLHF + 가드레일 조합이 **96% 환각 감소**를 달성했다는 결과는, 단일 기법에 의존하는 것이 아닌 **다층 방어(defense-in-depth) 전략**의 효과를 입증한다.

**실무적 구현 우선순위**:
1. **즉시 적용 가능**: 프롬프트 엔지니어링(CoT, 출처 명시, 불확실성 허용)
2. **중기 투자**: RAG 파이프라인 구축 + 하이브리드 검색 + 리랭킹
3. **장기 투자**: 도메인 특화 미세조정, DPO/RLHF 정렬
4. **지속적 운영**: 환각 탐지 모니터링, 사용자 피드백 루프

이 논문이 제시하는 체계적 분류체계는 각 조직의 상황과 자원에 맞는 **맞춤형 환각 완화 로드맵** 수립을 위한 실질적 프레임워크로 활용될 수 있다.