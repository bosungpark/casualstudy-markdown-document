# Safeguarding Large Language Models: A Survey

## 출처
- **링크**: https://arxiv.org/abs/2406.02622
- **저자**: Yi Dong, Ronghui Mu, Yanghao Zhang et al. (University of Liverpool)
- **게재**: 2024년 6월 제출 (arXiv cs.CR)

---

## AI 요약

이 서베이 논문은 LLM의 "가드레일(Guardrails)" 또는 "세이프가드(Safeguards)"라 불리는 안전 메커니즘의 현재 상태, 공격 기법, 방어 전략을 체계적으로 정리한 종합 문헌 연구다. 가드레일은 RLHF 같은 모델 내부 학습과 달리 **블랙박스 사후 전략**으로, 학습된 LLM의 입출력을 모니터링하고 필터링하는 외부 알고리즘을 의미한다. 가드레일이 보호해야 할 속성으로는 환각(hallucination), 공정성(fairness), 프라이버시(privacy), 견고성(robustness), 독성(toxicity), 합법성(legality), OOD 탐지, 불확실성(uncertainty) 등 8가지가 다뤄진다.

**가드레일 프레임워크 분류**: 주요 가드레일 시스템들은 Type-1(심볼릭 입출력을 받는 학습 에이전트) 또는 Type-2(심볼릭 알고리즘이 학습 알고리즘을 지원) 신경-심볼릭 시스템으로 분류된다. **Llama Guard**는 Llama2-7b 기반 분류 모델로 입출력을 사용자 정의 카테고리로 분류해 zero/few-shot 적응이 가능하다. **Nvidia NeMo**는 Colang 실행 언어를 사용해 대화 흐름을 제어하며, KNN 기반 임베딩 검색으로 프롬프트를 canonical form과 매칭한 후 LLM을 여러 단계에서 호출(유저 의도 추론 → 다음 단계 생성 → 봇 메시지 생성)하여 사전 구현된 모더레이션(fact-checking, 환각 방지)을 제공한다. **Guardrails AI**는 RAIL 스펙(XML 형식)으로 출력 형식/타입을 정의하고, Guard를 초기화한 뒤 LLM을 래핑해 에러 발생 시 자동으로 재생성 프롬프트를 생성한다. TruLens는 RAG 평가 중심으로 컨텍스트 관련성, 답변 관련성, groundedness를 측정하며, Guidance AI와 LMQL은 템플릿 언어로 생성 흐름에 제약(regex, CFG)을 삽입해 토큰 확률 분포를 조작하고 토큰 힐링(token healing)으로 경계를 최적화한다. Python 패키지로는 LangChain(체인 구성), AIF360(편향 탐지), ART(적대 공격 방어), Fairlearn(편향 완화), Detoxify(독성 콘텐츠 필터링)가 소개된다.

**환각(Hallucination)**: 환각은 실재하지 않거나 비논리적이거나 프롬프트와 무관한 응답을 생성하는 현상으로, 데이터 소싱, 사전 학습, 정렬, 추론 단계에서 발생한다. 탐지 기법으로는 CI(Continuous Integration)를 활용한 자동 테스트, 신뢰 정보 소스 교차 검증, entity/relation 기반 fact 메트릭, NLI 기반 분류기, LLM 자체 평가(binary judgment, Likert scale) 등이 있다. NeMo는 원본 응답을 가설로, 추가 생성한 2개 응답을 증거로 사용해 NLI 원리로 일관성을 검증한다. SMT(satisfiability modulo theory) 솔버를 이용한 형식 검증 프레임워크도 제안되었으며, 이는 환각 출력을 제거하고 올바른 출력으로 유도하는 반례 유도 합성(counterexample-guided inductive synthesis)을 수행한다.

**공정성(Fairness)**: 편향은 임베딩에 인코딩되어 downstream 태스크로 전파되며(예: CEO=He, Secretary=She), 성별, 인종, 문화, 사회적 편향으로 나타난다. 완화 방법은 **내재적 탈편향(intrinsic debiasing)**과 **외재적 탈편향(extrinsic debiasing)**으로 나뉜다. 내재적 방법으로는 Counterfactual Data Augmentation(CDA)를 통한 레이블 균형화, FairBERTa 같은 대규모 교란 데이터셋 재학습, contrastive learning 기반 탈편향 모듈 추가가 있다. 외재적 방법으로는 DAMA(causal analysis로 편향 전달 컴포넌트 식별 후 mid-upper FFN 계층 조정), LoRA 기반 탈편향, prompt engineering(문화별 프롬프팅, 성별 편향 억제 preamble), in-context learning을 통한 공정한 콘텐츠 생성이 있다. 가드레일 구축 시 학습 데이터 모니터링/필터링, 알고리즘 파라미터 조정, 편향 탐지 도구 통합, 지속적 학습(새 데이터/사회 규범 업데이트)이 필요하다. 공정성 평가는 point-based가 아닌 distribution-based로 사후 분포 추정 및 거리 측정이 권장된다.

**프라이버시(Privacy)**: GDPR, CCPA 등 법률 준수가 필수이며 PII(personally identifiable information) 유출 방지가 핵심이다. 방어 기술로는 Differential Privacy(DP)가 주류로, DP-SGD 적용 시 트릭이 필요하며(성능 저하 방지), 50% sparsity에서도 성능 유지가 가능하다. Local Differential Privacy(LDP) 기반 텍스트 재작성, PPLM(Privacy Protection Language Models)의 도메인별 지식 통합, Silent Guardian의 Truncation Protection Examples, prefix prompt 학습으로 memorization 최소화, JFT(Just Fine-tune Twice) 프레임워크를 통한 Selective Differential Privacy 달성 등이 제안되었다. **워터마킹**은 저작권 보호에 중요하며, 생성 전 "green" 토큰 세트를 랜덤화하고 샘플링 시 선호도를 부여해 watermark를 감지한다. 사용자가 개인 문서에 워터마크를 삽입하면 LLM 개발자는 학습 데이터에 워터마크 포함 여부를 자동 검증하고, **model unlearning**으로 개인 텍스트를 제거해야 한다. 기존 앱에는 blocklist/allowlist/suppression list를 통한 콘텐츠 제어, human-in-the-loop 검토, 모델 재학습, NeMo의 허용된 third-party 앱 제한이 적용된다. 개발 단계에서는 윤리적 리스크 평가, selective memory, PII 제거, 최신 LLM 버전 업데이트가 필요하다.

**견고성(Robustness)**: 적대적 견고성은 의도적으로 조작된 입력(typo, 대소문자 변경, 동의어 교체 등)에 대한 성능 유지를 의미한다. 가드레일은 입력 전처리(typo 수정, 포맷 표준화)로 적대 콘텐츠를 중화하고, 출력 모니터링(임계값 설정, 패턴 이탈 플래깅)으로 이상 응답을 전문 보안팀에 전달한다. **독성(Toxicity)**: PerspectiveAPI 같은 분류기 점수로 측정되지만 평가자 편향과 인코더 교란에 취약하다. 역할 할당에 따라 독성 출력이 크게 달라지며, 암묵적 독성은 zero-shot 방법으로 탐지하기 어렵다. NeMo는 사용자가 독성 출력을 정의하고 워크플로우를 설정해 잠재적 독성 콘텐츠 노출 시 챗봇이 지원하도록 한다. **합법성(Legality)**: 모델 개발 단계에서 학습 데이터 스크리닝/정제, RLHF를 통한 피드백 기반 행동 수정, 출시 전 윤리/안전 테스트(Red Teaming: Google의 internal red team, OpenAI의 external red team network)가 수행된다. 모니터링 시스템은 NLP와 anomaly detection으로 실시간 탐지하며, Google의 PaLM-based Moderation(16+ 유형 탐지), OpenAI Moderation API, Anthropic의 Constitutional AI, Meta의 Llama Guard를 활용한다. LangChain의 ConstitutionalChain은 사전 정의된 규칙으로 생성 콘텐츠를 필터링/수정해 헌법 원칙에 맞춘다. **OOD 탐지**: 학습 데이터와 다른 차원의 데이터로 DNN이 과신(overconfident) 결정을 내리는 문제다. LLM의 방대한 학습 코퍼스로 인해 OOD 정의가 어렵지만, 특정 응용(텍스트 분류, 감정 분석, MRC)에서는 task 무관 데이터나 정상 데이터에서 크게 벗어난 데이터로 정의 가능하다. anomaly input filtering이나 task-specific OOD detector 구축으로 대응한다. **불확실성(Uncertainty)**: 모델이 자신의 예측 신뢰도를 파악하는 능력으로, 높은 불확실성은 출력 거부나 추가 검토가 필요함을 의미한다. Semantic Entropy는 동일한 의미를 가진 문장들을 클러스터링하고 의미론적 등가 관계를 사용해 불확실성을 측정한다(H(C|x) = -ΣP(C|x)lnP(C|x)). 여러 reasoning chain 결합, 다양한 프롬프트 통합, 모델이 직접 confidence를 출력하도록 유도하는 방법이 calibration을 개선한다. KnowNo 프레임워크는 LLM planner의 불확실성을 측정/정렬해 한계를 인식하고 도움을 요청하게 한다.

**공격 기법(Jailbreaks)**: 가드레일을 우회하는 jailbreak는 white-box, black-box, grey-box로 분류된다. **White-box**: GCG(Greedy Coordinate Gradient)는 연속 완화와 그리디 검색으로 adversarial suffix를 찾아 모델이 "Sure, this is..." 같은 긍정 응답을 생성하도록 유도하며, PGD는 error 제어로 GCG 대비 10배 빠르다. AutoDAN-Zhu는 GCG의 이중 루프 최적화로 perplexity filter를 우회하는 stealthy prompt를 생성하고, COLD-Attack은 fluency, stealthiness, sentiment, left-right-coherence 제약 하에 연속 logit 공간에서 샘플링한다. PRP(Prefix-based attack)는 가드레일 모델을 공격하는 2단계 prefix 방법으로, universal adversarial prefix 구축 후 response에 전파해 가드레일이 유해 콘텐츠를 출력하게 만든다. AutoDAN-Liu는 hierarchical genetic algorithm으로 윤리 가이드라인을 우회하는 prompt를 자연 선택 기반으로 반복 개선한다. ProMan은 오픈소스 LLM의 생성 과정을 직접 조작해 특정 위치에 특정 토큰을 생성하도록 강제한다. **Black-box**: JailBroken은 competing training objectives와 instruction tuning objectives의 failure mode를 활용한 objectionable prompt를 제작한다. DeepInception은 Milgram shock experiment에서 영감을 받아 nested scene을 만들어 LLM을 "hypnotize"하는 inception 메커니즘을 주입한다. DAN(Do Anything Now)은 LLM이 제약 없이 모든 기능을 수행하도록 캐릭터화한다. ICA(In-Context Attack)는 in-context learning으로 유해 출력을 생성하도록 악의적 컨텍스트를 구축하고, SAP(Semi-Automatic Attack Prompt)는 고품질 프롬프트를 초기 세트로 삼아 in-context learning으로 반복 업데이트한다. DRA(Disguise and Reconstruction Attack)는 유해 명령을 위장해 입력 필터를 우회하고 모델이 원본 유해 명령을 재구성하게 만든다. CipherChat은 cipher/low-resource language로 쿼리를 인코딩하고, MultiLingual은 비영어 언어 번역으로 안전 장치를 우회하며, CodeChameleon은 reverse order, word length, odd/even position, binary tree 기반 암호화 함수로 의도 인식을 회피한다. ReNeLLM은 Prompt Rewriting + Scenario Nesting으로 자동 jailbreak 생성을 일반화하고, PAIR(Prompt Automatic Iterative Refinement)는 attacker LM이 이전 프롬프트/응답에서 학습해 judge score 기반으로 새 프롬프트를 개선하며, GPTFUZZER는 AFL fuzzing에서 영감을 받아 seed selection, mutate operators, judgment model로 취약점을 자동 탐지한다. TAP(Tree of Attacks with Pruning)는 tree-of-thought reasoning으로 candidate prompt를 반복 개선하고, GA(Genetic Algorithm)는 random subset sampling으로 fitness를 근사해 benign input embedding과 adversarial input embedding의 cosine similarity를 최소화한다. GUARD는 4개 역할을 할당한 role-playing system으로 기존 jailbreak를 지식 그래프로 수집하고 Chain-of-Thought로 새 jailbreak를 생성한다. **Prompt Injection**: PROMPTINJECT는 mask 기반 iterative strategy로 goal hijacking(악의 문자열로 특정 시퀀스 생성 유도)과 prompt leaking(비밀 값 노출)을 수행한다. IPI(Indirect Prompt Injection)는 retrieved prompt를 "arbitrary code"로 활용해 Bing Chat, code-completion engine, GPT-4를 공격하고, HOUYI는 SQL injection/XSS 공격 기법을 차용해 prompt abuse와 prompt leak을 수행한다. Mosaic Prompts는 semantic censorship의 한계를 악용해 여러 permissible output을 조합해 impermissible output을 구성한다. CIA(Compositional Instruction Attack)는 여러 요소로 구성된 명령에서 유해 의도를 탐지하지 못하는 취약점을 활용하며, T-CIA는 심리 원칙으로 페르소나를 정렬하고 W-CIA는 유해 프롬프트를 창작 과제로 위장한다. **Grey-box**: fine-tuning attack은 15개 유해 또는 100개 benign 예제로 GPT-4의 safeguard를 무력화할 수 있으며(95% 성공률), instruction tuning 과다는 여전히 유해 콘텐츠를 생성한다. Janus는 PII recovery task 정의 후 few-shot fine-tuning으로 GPT-3.5가 10개 PII 예제로도 PII 노출 가능성을 크게 높인다. RAG 공격은 외부 데이터셋에 악의 명령을 주입해 ChatGPT의 안전 보호를 무효화하며, 5개 독성 텍스트 주입으로 90% 공격 성공률을 달성한다. Backdoor attack은 학습 데이터에 trigger를 삽입해 특정 입력 시 악의적 동작을 수행하게 하며, AutoPoison은 목표 콘텐츠를 참조하는 학습 예제를 주입하고, LoFT는 local proxy model을 fine-tuning해 공격 전이성을 높이며, BadGPT는 RL fine-tuning 단계에서 reward model에 backdoor를 주입하고, ICLAttack은 in-context learning을 겨냥해 demonstration set과 prompt를 조작하며, activation steering은 학습 데이터 오염 없이 truthfulness, toxicity, bias, harmfulness에 영향을 준다.

**방어 기술**: Detection-based 방법으로 PPL(perplexity)은 입력의 perplexity를 계산해 요청 수락/거부를 결정하고, SmoothLLM은 randomized smoothing으로 입력을 여러 번 변경해 예측을 결합해 적대 입력을 식별하며, ICD(In-Context Defense)는 유해 프롬프트 거부 시연을 in-context demonstration으로 제공해 모델 복원력을 강화한다. LLM SELF DEFENSE는 생성 콘텐츠를 사전 정의 프롬프트에 삽입하고 다른 LLM 인스턴스로 분석해 추가 가드레일 필터를 구축하며, RA-LLM(Robustly Aligned LLM)은 요청에 여러 랜덤 드롭을 추가해 Robust Alignment Check Function으로 검증하고, MTD(Moving Target Defense)는 각 응답의 quality + toxicity 복합 점수를 계산해 랜덤화로 적격 응답을 선택한다. Mitigation-based 방법으로 Retokenization과 Paraphrase는 GCG 같은 공격을 무력화하며, RAIN(Rewindable Auto-regressive INference)은 사전 학습 LLM이 자신의 출력을 평가하고 평가 결과로 backtracking/생성을 유도해 AI 안전성을 향상시킨다(RLHF 불필요, 추가 모델 유지 없음, 그래디언트 데이터/계산 그래프 축적 안 함). GP(Goal Prioritization)는 helpfulness와 safety 목표 간 충돌을 분석해 추론 단계나 학습+추론에 목표 우선순위를 적용해 jailbreak 성공률을 크게 낮춘다. Self-Reminder는 가장 외부 수준에서 "system mode" 프롬프트를 추가해 책임 있는 AI 도구로서 역할을 상기시켜 악의적 유도에 대한 취약성을 줄이며, MART(Multi-round Automatic Red-teaming)는 adversarial LLM과 target LLM이 반복적으로 상호작용하며 target LLM을 안전 데이터로 개선한다. RPO(Robust Prompt Optimization)는 gradient-based token optimization(GCG 유사)으로 무해한 출력을 보장하는 첫 적대적 목표와 알고리즘을 제안하며, SafeDecoding은 학습 단계에서 소수의 안전 조치로 fine-tuning하고 추론 시 토큰 분포를 재구성해 공격자 목표에 공명하는 토큰 확률을 낮추고 인간 가치에 부합하는 토큰 확률을 높인다.

**전체 가드레일 구축을 위한 논의**: **충돌하는 요구사항(Conflicting Requirements)**은 safety vs intelligence, fairness vs privacy, privacy vs robustness, robustness vs fairness 등이 있으며, 가드레일 도입은 ChatGPT의 응답 길이를 600자에서 140자로 급감시키는 보수적 전환을 야기한다. creativity 평가(Consensual Assessment Technique: fluency, flexibility, originality, elaboration)를 가드레일 개발에 통합하는 것이 제안되며, "to do or not to do" 결정(모든 민감 질문 거부 vs 더 지능적 접근)이 중요하다. **다학제 접근(Multidisciplinary Approach)**은 domain-specific LLM의 특수 규칙이 일반 원칙과 충돌할 수 있고(범죄 예방에서 'gun', 'crime' 용어 사용 필요), fairness/toxicity 같은 요구사항이 맥락 없이 정확히 정의하기 어렵다는 점을 지적한다. socio-technical theory와 whole system approach(지역 이해관계자가 통합 솔루션을 위해 모이는 동적/지속적 작업 방식)를 적용하는 다학제 전문가 그룹이 구체적 요구사항을 과학적으로 결정하고 정당화/검증해야 한다. **Neural-Symbolic 구현**: 기존 가드레일(RAIL, Colang)은 각 규칙을 독립적으로 적용하나 규칙 충돌 처리가 불명확하고, 시간/시나리오/데이터셋에 따른 의미론적 변화에 충분히 유연/적응적이지 않다. logic + decision theory 결합의 원칙적 접근이 요구사항 충돌 해결에 필요하며, symbolic + learning-based 방법의 협력이 필수적이다. **Systems Development Life Cycle(SDLC)**: V-model을 통한 각 개발 프로세스와 테스트 활동 연결이 최종 제품 품질 보장에 도움이 되며, 엄격한 검증/테스트가 필요하다. 개별 요구사항에는 randomized smoothing 같은 통계적 보증 인증이 유용하고, 다중 충돌 요구사항에는 Pareto front 기반 평가(non-dominated solution 집합)와 단일 요구사항 통계적 인증을 결합해야 한다. Pareto front는 모든 목표 고려 시 더 나은 해가 없는 해 집합을 나타내며, 통계적 인증은 불확실성/변동성 존재 시 특정 신뢰 수준으로 요구사항 충족을 보장한다. randomized smoothing이 fairness 문제를 야기하는 이론적 한계에도 주의가 필요하다. **LLM Agents 보호**: LLM agents는 LLM의 능력을 확장해 의사결정/행동 개시 능력을 통합하며 5개 기본 모듈(LLMs, planning, action, external tools, memory/knowledge)로 구성된다. 자율성으로 인해 복잡성/예측 불가능성이 증가하며, 다양한 도구/환경과 상호작용하는 에이전트의 안전은 종종 간과되어 유해 출력(ToolEmu, AgentMonitor, R-Judge 연구)이 발생한다. safeguard는 더 엄격한 제어와 감독으로 광범위한 능력을 효과적으로 관리해야 한다.

**결론**: 가드레일은 LLM과 인간 간 상호작용을 관리하는 복잡한 메커니즘으로, 다학제 팀의 체계적 접근으로 복잡성을 완전히 고려하고 최종 제품에 보증을 제공할 수 있다. 기존 가드레일은 특정 요구사항에 효과적이지만 모든 잠재적 충돌을 해결하는 범용 솔루션은 아니며, 특정 맥락에서 더 타겟팅된 접근이 더 나은 충돌 해결을 제공할 수 있다. 지속적인 연구로 구체적 원칙, 방법, 표준을 개발하고 다학제 팀이 구현/준수해야 한다.

---

## 내가 얻은 인사이트

1. **가드레일은 외부 레이어 방어 전략이다**: RLHF 같은 모델 내부 학습과 달리 가드레일은 블랙박스 사후 전략으로 입출력을 모니터링/필터링하는 알고리즘이다. 이는 모델을 변경하지 않고 배포 단계에서 안전을 보장하며, Llama Guard(분류), NeMo(Colang 흐름 제어), Guardrails AI(RAIL 스펙 검증), Guidance/LMQL(템플릿 제약) 등 다양한 프레임워크가 Type-1/Type-2 신경-심볼릭 시스템으로 구현된다. 이는 "순수 가드레일(모델 바꾸지 않고 바깥 레이어 제어)" 개념을 정확히 보여준다.

2. **8가지 보호 속성의 기술별 차이가 명확하다**: 환각은 CI/NLI/SMT 검증으로, 공정성은 CDA/DAMA/prompt engineering으로, 프라이버시는 DP/워터마킹/unlearning으로, 견고성은 입력 전처리/출력 모니터링으로, 독성은 PerspectiveAPI/역할별 정의로, 합법성은 Red Teaming/Moderation API로, OOD는 anomaly filtering/task-specific detector로, 불확실성은 Semantic Entropy/KnowNo로 각각 대응한다. 이는 환각 탐지(RAG-HAT, Semantic Entropy, LLM-Check, Entity Detection)와는 다른 broader safety 관점을 제공한다.

3. **Jailbreak 공격 분류 체계가 실용적이다**: White-box(GCG, PGD, AutoDAN, ProMan)는 그래디언트 기반 최적화로 adversarial suffix/prefix를 생성하고, Black-box(DeepInception, DAN, CipherChat, PAIR, GPTFUZZER, TAP)는 nested scene, cipher, fuzzing, tree-of-thought로 우회하며, Grey-box(fine-tuning, RAG poisoning, backdoor)는 학습 데이터 조작으로 15개 예제만으로도 95% 성공률을 달성한다. Stealthiness(high/medium/low), GPT4 evaluation, targeted property(toxicity/privacy/fairness/hallucination) 비교표가 각 공격의 특성을 한눈에 파악하게 한다.

4. **방어 기술은 Detection + Mitigation 이중 전략이다**: Detection(PPL, SmoothLLM, ICD, RA-LLM, MTD)은 입력/출력 필터링과 anomaly 탐지로 공격을 차단하고, Mitigation(RAIN, GP, Self-Reminder, MART, RPO, SafeDecoding)은 생성 과정 자체를 개선해 유해 출력을 원천 차단한다. RAIN의 backtracking, GP의 목표 우선순위, SafeDecoding의 토큰 분포 재구성은 RLHF 없이도 안전성을 향상시키는 추론 단계 해법을 제시한다.

5. **완전한 가드레일 구축은 multidisciplinary + SDLC + neural-symbolic이 핵심이다**: 충돌하는 요구사항(safety vs intelligence, fairness vs privacy)은 Pareto front + 통계적 인증 결합으로 trade-off를 찾고, 도메인별 특수 규칙은 socio-technical theory + whole system approach로 다학제 전문가가 정당화하며, 규칙 충돌은 logic + decision theory로 해결하고, SDLC의 V-model로 각 개발-테스트 단계를 연결해 품질을 보장해야 한다. LLM Agents의 자율성 증가로 더 엄격한 제어(ToolEmu, AgentMonitor, R-Judge)가 필요하며, 가드레일은 특정 맥락에 타겟팅된 솔루션이지 범용 해결책은 아니다.
