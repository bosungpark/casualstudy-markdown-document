# A Survey on Evaluation of Large Language Models

## 출처
- **링크**: https://arxiv.org/abs/2307.03109
- **저자**: Yupeng Chang, Xu Wang, Jindong Wang 외 13명 (Jilin University, Microsoft Research, CMU 등)
- **발표**: 2023년 7월 (ACM TIST 2024 게재)
- **GitHub**: https://github.com/MLGroupJLU/LLM-eval-survey

---

## AI 요약

### 논문 개요
이 Survey는 LLM 평가를 **What (무엇을), Where (어디서), How (어떻게)** 세 가지 차원에서 체계적으로 정리한다. 이전 Survey들이 LLM의 "능력"과 "내부 작동"을 다뤘다면, 이 논문은 "그 능력을 어떻게 측정하는가"에 집중한다.

### 핵심 구조: 평가의 3차원
```
┌─────────────────────────────────────────────────────────────┐
│              LLM 평가의 3차원 프레임워크                      │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  [WHAT] 무엇을 평가할 것인가?                                │
│  ├─ NLP Tasks (NLU, NLG, 번역, QA)                          │
│  ├─ Reasoning (수학, 논리, 상식)                            │
│  ├─ Domain-specific (의료, 법률, 금융, 교육)                │
│  ├─ Ethics & Safety (편향, 독성, 사실성)                    │
│  └─ Agent Applications (도구 사용, 계획)                    │
│                                                              │
│  [WHERE] 어디서 평가할 것인가?                               │
│  ├─ Benchmarks (MMLU, BIG-bench, HELM)                      │
│  ├─ Datasets (SQuAD, GSM8K, HumanEval)                      │
│  └─ Leaderboards (Open LLM, Chatbot Arena)                  │
│                                                              │
│  [HOW] 어떻게 평가할 것인가?                                 │
│  ├─ Automatic Metrics (Accuracy, BLEU, ROUGE)               │
│  ├─ Human Evaluation (선호도, 품질 평가)                    │
│  └─ LLM-as-Judge (GPT-4로 평가)                             │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 1. WHAT: 평가 대상 분류

#### 1.1 자연어 처리 태스크

| 범주 | 태스크 | 대표 벤치마크 |
|------|--------|--------------|
| **NLU** | 감정분석, 텍스트분류, NLI | GLUE, SuperGLUE |
| **NLG** | 요약, 대화, 번역 | CNN/DailyMail, WMT |
| **QA** | 독해, 지식 질의응답 | SQuAD, TriviaQA |
| **Multilingual** | 다국어 이해/생성 | XTREME, mGPT |

#### 1.2 추론 능력
```
┌─────────────────────────────────────────────────────────────┐
│                    추론 능력 평가 분류                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  [수학적 추론]                                               │
│  ├─ GSM8K: 초등 수학 문장제                                 │
│  ├─ MATH: 고급 수학 (경시대회 수준)                         │
│  └─ SVAMP: 수학 문제 변형 강건성                            │
│                                                              │
│  [논리적 추론]                                               │
│  ├─ LogiQA: 논리 문제                                       │
│  ├─ FOLIO: 1차 논리 추론                                    │
│  └─ bAbI: 기본 추론 태스크                                  │
│                                                              │
│  [상식 추론]                                                 │
│  ├─ HellaSwag: 문장 완성                                    │
│  ├─ PIQA: 물리적 상식                                       │
│  ├─ CommonsenseQA: 일반 상식                                │
│  └─ ARC: 과학 상식 (초등학교 수준)                          │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

#### 1.3 도메인별 평가

| 도메인 | 평가 포인트 | 대표 벤치마크 |
|--------|------------|--------------|
| **의료** | 진단 정확도, 의료 지식 | PubMedQA, MedQA |
| **법률** | 법적 추론, 판례 분석 | LegalBench |
| **금융** | 재무 분석, 리스크 평가 | PIXIU |
| **교육** | 교육 콘텐츠 생성, 채점 | MMLU (교육 과목) |
| **과학** | 과학 논문 이해, 실험 설계 | SciFact, ScienceQA |

#### 1.4 윤리 및 안전성
```
┌─────────────────────────────────────────────────────────────┐
│                  윤리/안전성 평가 차원                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  [편향 (Bias)]                                               │
│  ├─ BBQ: 질의응답에서의 편향                                │
│  ├─ BOLD: 개방형 생성에서의 편향                            │
│  └─ WinoBias: 성별 편향                                     │
│                                                              │
│  [독성 (Toxicity)]                                           │
│  ├─ RealToxicityPrompts: 독성 콘텐츠 생성 유도              │
│  ├─ ToxicChat: 실제 대화에서의 독성                         │
│  └─ SafetyBench: 안전성 종합 평가                           │
│                                                              │
│  [사실성 (Factuality)]                                       │
│  ├─ TruthfulQA: 허위정보 생성 경향                          │
│  ├─ FActScore: 사실 정확도 점수                             │
│  └─ HaluEval: 환각 평가                                     │
│                                                              │
│  [강건성 (Robustness)]                                       │
│  ├─ PromptBench: 적대적 프롬프트 저항성                     │
│  ├─ AdvGLUE: 적대적 공격 저항성                             │
│  └─ CheckList: 행동 테스트                                  │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 2. WHERE: 주요 벤치마크 & 데이터셋

#### 2.1 종합 벤치마크 비교

| 벤치마크 | 특징 | 태스크 수 | 주요 평가 차원 |
|---------|------|----------|--------------|
| **MMLU** | 57개 학문 분야 | 15,908 | 지식, 추론 |
| **BIG-bench** | 204개 다양한 태스크 | 204 | 창발적 능력 |
| **HELM** | 7개 메트릭 × 다수 시나리오 | 42 | 종합적 투명성 |
| **SuperGLUE** | 어려운 NLU 태스크 | 8 | 언어 이해 |
| **Open LLM Leaderboard** | 6개 핵심 벤치마크 통합 | 6 | 모델 비교 |

#### 2.2 MMLU 상세
```
┌─────────────────────────────────────────────────────────────┐
│            MMLU (Massive Multitask Language Understanding)   │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  구조: 57개 과목 × 4지선다 문제                             │
│  난이도: 고등학교 ~ 전문가 수준                              │
│  문항 수: ~15,908개                                          │
│                                                              │
│  분야별 구성:                                                │
│  ├─ STEM: 수학, 물리, 화학, 컴퓨터 과학                     │
│  ├─ 인문학: 역사, 철학, 문학                                │
│  ├─ 사회과학: 경제, 법률, 심리학                            │
│  └─ 전문분야: 의학, 법학, 회계                              │
│                                                              │
│  평가 방식:                                                  │
│  • 정답률 = 맞은 문항 / 전체 문항                           │
│  • Few-shot (0, 5-shot) 설정                                │
│  • 과목별 점수 + 전체 평균                                  │
│                                                              │
│  한계:                                                       │
│  • 데이터 오염 (훈련 데이터에 포함 가능성)                  │
│  • 영어/서구 중심                                           │
│  • 정적 데이터셋 (업데이트 어려움)                          │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

#### 2.3 HELM (Holistic Evaluation of Language Models)
```
7가지 핵심 메트릭:
┌────────────────┬─────────────────────────────────────────┐
│ 메트릭         │ 설명                                     │
├────────────────┼─────────────────────────────────────────┤
│ Accuracy       │ 정확도                                   │
│ Robustness     │ 입력 변형에 대한 강건성                  │
│ Calibration    │ 신뢰도와 정확도의 일치도                 │
│ Fairness       │ 그룹 간 성능 차이                        │
│ Bias           │ 사회적 편향                              │
│ Toxicity       │ 유해 콘텐츠 생성 경향                    │
│ Efficiency     │ 연산 효율성                              │
└────────────────┴─────────────────────────────────────────┘
```

### 3. HOW: 평가 방법론

#### 3.1 자동 평가 메트릭

| 메트릭 | 측정 대상 | 적용 태스크 | 한계 |
|--------|----------|------------|------|
| **Accuracy** | 정답 일치율 | 분류, 객관식 | 부분 정답 무시 |
| **BLEU** | n-gram 일치 (정밀도) | 번역, 생성 | 의미적 유사성 무시 |
| **ROUGE** | n-gram 일치 (재현율) | 요약 | 동의어 처리 약함 |
| **BERTScore** | 임베딩 유사도 | 생성 전반 | 계산 비용 높음 |
| **Perplexity** | 예측 불확실성 | 언어 모델링 | 품질과 직접 연관 약함 |
| **Pass@k** | 코드 통과율 | 코드 생성 | 테스트 케이스 의존 |

#### 3.2 인간 평가
```
┌─────────────────────────────────────────────────────────────┐
│                     인간 평가 방법                           │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  [직접 평가]                                                 │
│  ├─ Likert Scale: 1-5점 척도로 품질 평가                    │
│  ├─ A/B Test: 두 출력 중 선호도 선택                        │
│  └─ Pairwise Comparison: 쌍별 비교로 순위 결정              │
│                                                              │
│  [Chatbot Arena]                                             │
│  ├─ 크라우드소싱 기반 실시간 평가                           │
│  ├─ 익명 모델 대결 → 사용자 투표                           │
│  ├─ Elo Rating으로 순위 결정                                │
│  └─ 장점: 실제 사용자 선호도 반영                          │
│                                                              │
│  한계:                                                       │
│  • 비용과 시간 소요                                         │
│  • 평가자 간 일관성 문제                                    │
│  • 확장성 제한                                              │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

#### 3.3 LLM-as-Judge (GPT-4 등을 평가자로 활용)
```
┌─────────────────────────────────────────────────────────────┐
│                    LLM-as-Judge 패러다임                     │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  방식:                                                       │
│  ├─ Single Answer: 단일 출력 품질 평가                      │
│  ├─ Pairwise: 두 모델 출력 비교                             │
│  └─ Reference-guided: 참조 답변 대비 평가                   │
│                                                              │
│  대표 프레임워크:                                            │
│  ├─ AlpacaEval: GPT-4 기반 자동 평가                        │
│  ├─ MT-Bench: 다회전 대화 평가                              │
│  └─ JudgeBench: LLM Judge 평가용 벤치마크                   │
│                                                              │
│  장점:                                                       │
│  • 확장성 (대규모 평가 가능)                                │
│  • 비용 효율성                                              │
│  • 재현 가능성                                              │
│                                                              │
│  한계:                                                       │
│  • Position Bias (순서에 따른 편향)                         │
│  • Verbosity Bias (긴 답변 선호)                            │
│  • Self-enhancement Bias (자기 모델 선호)                   │
│  • 프롬프트에 민감                                          │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 4. 평가의 핵심 도전과제

#### 4.1 데이터 오염 (Data Contamination)
```
문제: 벤치마크 데이터가 훈련 데이터에 포함

증거:
- GPT-4가 MMLU 마스킹된 선택지를 52-57% 정확도로 추측
  (무작위 = 25%)
- 훈련 데이터 공개 안 함 → 오염 여부 확인 불가

대응:
- 동적 벤치마크 (LLMEval-3: 220k 문제 은행에서 샘플링)
- 시간 기반 분리 (출시 후 생성된 문제만 사용)
- 오염 감지 테스트 (fill-in-the-blank replay test)
```

#### 4.2 재현성 문제

| 문제 | 설명 | 예시 |
|------|------|------|
| **구현 차이** | 같은 벤치마크, 다른 결과 | MMLU: Eleuther vs HELM vs 원본 |
| **프롬프트 민감성** | 프롬프트 변형 → 성능 변화 | 2% 이상 점수 변동 가능 |
| **설정 차이** | Few-shot 수, 온도 등 | 5-shot vs 0-shot 큰 차이 |

#### 4.3 벤치마크 포화 (Saturation)
```
진화 과정:
GLUE (2018) → 빠르게 포화 → SuperGLUE (2019) → 포화
→ MMLU (2020) → 90%+ 도달 중 → MMLU-Pro (2024)

문제:
- 모델이 벤치마크 "해킹" (진정한 능력 vs 벤치마크 최적화)
- 포화된 벤치마크는 모델 차별화 불가
- 새 벤치마크 지속 필요 → 비교 일관성 상실
```

### 5. 평가 방법 진화 트렌드
```
┌─────────────────────────────────────────────────────────────┐
│              LLM 평가 방법론의 진화                          │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  [1세대] 정적 벤치마크 (2018-2020)                          │
│  • GLUE, SuperGLUE, SQuAD                                   │
│  • 고정 데이터셋, 단일 메트릭                               │
│  • 문제: 빠른 포화, 데이터 오염                             │
│                                                              │
│  [2세대] 대규모 다중태스크 (2020-2022)                      │
│  • MMLU, BIG-bench, HELM                                    │
│  • 다양한 도메인, 복합 메트릭                               │
│  • 문제: 여전히 정적, 재현성 이슈                           │
│                                                              │
│  [3세대] 동적/인간 기반 (2022-2024)                         │
│  • Chatbot Arena, AlpacaEval                                │
│  • 실시간 평가, 인간 선호도 반영                            │
│  • 문제: 비용, 일관성                                       │
│                                                              │
│  [4세대] 자동화 + 강건성 (2024~)                            │
│  • LLMEval-3, LLM-as-Judge 고도화                          │
│  • 동적 문제 은행, 오염 저항                                │
│  • 다국어, 멀티모달 확장                                    │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 내가 얻은 인사이트
